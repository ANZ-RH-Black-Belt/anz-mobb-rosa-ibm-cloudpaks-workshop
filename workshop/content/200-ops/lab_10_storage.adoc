[sidebar]
.ROSA Environment Details
--
SSH to Bastion
[source,sh,role=execute]
----
ssh %rosa_bastion_user_name%@bastion.%rosa_subdomain_base%
----
Bastion SSH password
[source,sh,role=execute]
----
%rosa_user_password%
----
ROSA Console URL
[source,sh]
----
https://console-openshift-console.%rosa_subdomain_base%
----

ROSA Admin Password
[source,sh,role=execute]
----
echo $ADMIN_PASSWORD
----

Login string for OC Cli
[source,sh,role=execute]
----
oc login $API_SERVER --username cluster-admin --password $ADMIN_PASSWORD --insecure-skip-tls-verify=true
----

-- 

== Elastic File System (EFS)


== Enabling the AWS EFS CSI Driver Operator on ROSA

The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. With the release of OpenShift 4.10 the EFS CSI Driver is now GA and available.
This is a guide to quickly enable the EFS Operator on ROSA to a Red Hat OpenShift on AWS (ROSA) cluster with STS enabled.
Note: The official supported installation instructions for the EFS CSI Driver on ROSA are available link:https://access.redhat.com/articles/6966373[here] .

== Dynamic vs Static Provisioning

The CSI driver supports both Static and Dynamic provisioning. Dynamic provisioning should not be confused with the ability of the Operator to create EFS volumes.

=== Dynamic provisioning

Dynamic provisioning provisions new PVs as subdirectories of a pre-existing EFS volume. The PVs are independent of each other. However, they all share the same EFS volume. When the volume is deleted, all PVs provisioned out of it are deleted too. The EFS CSI driver creates an AWS Access Point for each such subdirectory. Due to AWS AccessPoint limits, you can only dynamically provision 120 PVs from a single StorageClass/EFS volume.

=== Static Provisioning

Static provisioning mounts the entire volume to a pod.

== Set up environment

Set some environment variables
[source,sh,role=execute]
----
export CLUSTER_NAME=rosa-${GUID}
echo "export CLUSTER_NAME=${CLUSTER_NAME}" >>$HOME/.bashrc
export AWS_REGION=$(aws configure get region)
echo "export AWS_REGION=${AWS_REGION}" >>$HOME/.bashrc
export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer | sed -e "s/^https:\/\///")
echo "export OIDC_PROVIDER=${OIDC_PROVIDER}" >>$HOME/.bashrc
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo "export AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}" >>$HOME/.bashrc
export SCRATCH_DIR=~/scratch
export AWS_PAGER=""
mkdir -p $SCRATCH_DIR
export FILE_STORAGECLASS=efs-sc
echo "export FILE_STORAGECLASS=${FILE_STORAGECLASS}" >>$HOME/.bashrc
export BLOCK_STORAGECLASS=gp3
echo "export BLOCK_STORAGECLASS=${BLOCK_STORAGECLASS}" >>$HOME/.bashrc
----

In order to use the AWS EFS CSI Driver we need to create IAM roles and policies that can be attached to the Operator.

1. Create an IAM Policy

[source,sh,role=execute]
----
cat << EOF > $SCRATCH_DIR/efs-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:DescribeAccessPoints",
        "elasticfilesystem:DescribeFileSystems",
        "elasticfilesystem:DescribeMountTargets",
        "ec2:DescribeAvailabilityZones"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:CreateAccessPoint"
      ],
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "aws:RequestTag/efs.csi.aws.com/cluster": "true"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:TagResource"
      ],
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": "elasticfilesystem:DeleteAccessPoint",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
        }
      }
    }
  ]
}
EOF
----

[start=2]
. Create the Policy

This creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler.

[source,sh,role=execute]
----
export POLICY=$(aws iam create-policy --policy-name "${CLUSTER_NAME}-rosa-efs-csi" --policy-document file://$SCRATCH_DIR/efs-policy.json --query 'Policy.Arn' --output text) || POLICY=$(aws iam list-policies --query "Policies[?PolicyName=='${CLUSTER_NAME}-rosa-efs-csi'].Arn" --output text)

echo "export POLICY=${POLICY}" >>$HOME/.bashrc
----

[start=3]
. Create a trust policy

[source,sh,role=execute]
----
cat <<EOF > $SCRATCH_DIR/TrustPolicy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "${OIDC_PROVIDER}:sub": [
            "system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator",
            "system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa"
          ]
        }
      }
    }
  ]
}
EOF
----

[start=4]
. Create Role for the EFS CSI Driver Operator

[source,sh,role=execute]
----
export ROLE=$(aws iam create-role --role-name "${CLUSTER_NAME}-aws-efs-csi-operator" --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json --query "Role.Arn" --output text)

echo "export ROLE=${ROLE}" >>$HOME/.bashrc

echo $ROLE
----

[start=5]
. Attach the Policies to the Role

[source,sh,role=execute]
----
aws iam attach-role-policy --role-name "${CLUSTER_NAME}-aws-efs-csi-operator" --policy-arn $POLICY
----

== Deploy the AWS EFS Operator

. Create a Secret to tell the AWS EFS Operator which IAM role to request.

[source,sh,role=execute]
----
cat << EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
 name: aws-efs-cloud-credentials
 namespace: openshift-cluster-csi-drivers
stringData:
  credentials: |-
    [default]
    role_arn = $ROLE
    web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token
EOF

----

[start=2]
. Install the EFS Operator

[source,sh,role=execute]
----
cat <<EOF | oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-cluster-csi-drivers-
  namespace: openshift-cluster-csi-drivers
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  labels:
    operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: ""
  name: aws-efs-csi-driver-operator
  namespace: openshift-cluster-csi-drivers
spec:
  channel: stable
  installPlanApproval: Automatic
  name: aws-efs-csi-driver-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

[start=3]
. Wait until the Operator is running

[source,sh,role=execute]
----
watch oc get deployment aws-efs-csi-driver-operator -n openshift-cluster-csi-drivers
----

.Sample Output of completed operator deployment
[source,texinfo,options=nowrap]
----
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
aws-efs-csi-driver-operator   1/1     1            1           2m35s
----

[start=4]
. Install the AWS EFS CSI Driver

[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: operator.openshift.io/v1
kind: ClusterCSIDriver
metadata:
    name: efs.csi.aws.com
spec:
  managementState: Managed
EOF
----

[start=5]
. Wait until the CSI driver is running

[source,sh,role=execute]
----
watch oc get daemonset aws-efs-csi-driver-node -n openshift-cluster-csi-drivers
----

.Sample Output of completed CSI driver deployment
[source,texinfo,options=nowrap]
----
NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
aws-efs-csi-driver-node   7         7         7       7            7           kubernetes.io/os=linux   42s
----

== Prepare an AWS EFS Volume for dynamic provisioning

1. Run this set of commands to update the VPC to allow EFS access

[source,sh,role=execute]
----
export NODE=$(oc get nodes --selector=node-role.kubernetes.io/worker -o jsonpath='{.items[0].metadata.name}')
echo "export NODE=${NODE}" >>$HOME/.bashrc

export VPC=$(aws ec2 describe-instances --filters "Name=private-dns-name,Values=$NODE" --query 'Reservations[*].Instances[*].{VpcId:VpcId}' --region $AWS_REGION | jq -r '.[0][0].VpcId')
echo "export VPC=${VPC}" >>$HOME/.bashrc

export CIDR=$(aws ec2 describe-vpcs --filters "Name=vpc-id,Values=$VPC" --query 'Vpcs[*].CidrBlock' --region $AWS_REGION | jq -r '.[0]')
echo "export CIDR=${CIDR}" >>$HOME/.bashrc

export SG=$(aws ec2 describe-instances --filters "Name=private-dns-name,Values=$NODE" --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' --region $AWS_REGION | jq -r '.[0][0].SecurityGroups[0].GroupId')
echo "export SG=${SG}" >>$HOME/.bashrc
----

[start=2]
. Assuming the CIDR and SG are correct, update the security group

[source,sh,role=execute]
----
aws ec2 authorize-security-group-ingress --group-id $SG --protocol tcp --port 2049 --cidr $CIDR | jq .
----

At this point you can create either a single Zone EFS filesystem, or a Region wide EFS filesystem

== Creating a region-wide EFS

1. Create a region-wide EFS File System

[source,sh,role=execute]
----
export EFS=$(aws efs create-file-system --creation-token efs-token-1 --region ${AWS_REGION} --encrypted | jq -r '.FileSystemId')
echo "export EFS=${EFS}" >>$HOME/.bashrc
----

[start=2]
. Configure a region-wide Mount Target for EFS (this will create a mount point in each subnet of your VPC by default)

[source,sh,role=execute]
----
for SUBNET in $(aws ec2 describe-subnets \
  --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \
  --query 'Subnets[*].{SubnetId:SubnetId}' \
  --region $AWS_REGION \
  | jq -r '.[].SubnetId'); do \
    MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \
       --subnet-id $SUBNET --security-groups $SG \
       --region $AWS_REGION \
       | jq -r '.MountTargetId'); \
    echo $MOUNT_TARGET; \
 done
echo "export MOUNT_TARGET=${MOUNT_TARGET}" >>$HOME/.bashrc
----

[start=3]
. Create a Storage Class for the EFS volume

[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: $FILE_STORAGECLASS
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: $EFS
  directoryPerms: "700"
  gidRangeStart: "1000"
  gidRangeEnd: "2000"
  basePath: "/dynamic_provisioning"
EOF
----

== (Optional) Testing the EFS Storage Class

1. Create a namespace

[source,sh,role=execute]
----
oc new-project efs-demo
----

[start=2]
. Create a PVC

[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-efs-volume
spec:
  storageClassName: $FILE_STORAGECLASS
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
EOF
----

[start=3]
. Create a Centos Pod to write to the EFS Volume

[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Pod
metadata:
 name: test-efs
spec:
 volumes:
   - name: efs-storage-vol
     persistentVolumeClaim:
       claimName: pvc-efs-volume
 containers:
   - name: test-efs
     image: centos:latest
     command: [ "/bin/bash", "-c", "--" ]
     args: [ "while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs && sleep 5; done;" ]
     volumeMounts:
       - mountPath: "/mnt/efs-data"
         name: efs-storage-vol
EOF

----

[NOTE]
====
It may take a few minutes for the pod to be ready. If you see errors such as Output: Failed to resolve "fs-XXXX.efs.us-east-2.amazonaws.com" it likely means its still setting up the EFS volume, just wait longer.
====

[start=4]
. Wait for the Pod to be ready

[source,sh,role=execute]
----
watch oc get pod test-efs
----

[start=5]
. Create a Pod to read from the EFS Volume

[source,sh,role=execute]
----
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Pod
metadata:
 name: test-efs-read
spec:
 volumes:
   - name: efs-storage-vol
     persistentVolumeClaim:
       claimName: pvc-efs-volume
 containers:
   - name: test-efs-read
     image: centos:latest
     command: [ "/bin/bash", "-c", "--" ]
     args: [ "tail -f /mnt/efs-data/verify-efs" ]
     volumeMounts:
       - mountPath: "/mnt/efs-data"
         name: efs-storage-vol
EOF

----

[start=6]
. Verify the second POD can read the EFS Volume

[source,texinfo,options=nowrap]
----
hello efs
hello efs
hello efs
hello efs
hello efs
hello efs
----

[start=7]
. Cleanup Storage test
[source,texinfo,role=execute]
----
oc delete project efs-demo
----